{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Notebook to build a new database of AI generations since I found out that LaMini Instruction contains human generated prompts/answers :clown: <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_datasets = {\"nomic-ai/gpt4all-j-prompt-generations\": \"response\", \n",
    "                    \"sahil2801/CodeAlpaca-20k\": \"output\", \n",
    "                    \"dkoterwa/ai_society_instructions\": \"output\",\n",
    "                    \"dkoterwa/camel_ai_biology_instruction_dataset\": \"response\",\n",
    "                    \"dkoterwa/camel_ai_physics_instruction_dataset\": \"response\",\n",
    "                    \"dkoterwa/camel_ai_chemistry_instruction_dataset\": \"response\",\n",
    "                    \"dkoterwa/camel_ai_maths_instruction_dataset\": \"response\",\n",
    "                    \"vicgalle/alpaca-gpt4\": \"output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, answer_column in english_datasets.items():\n",
    "    print(name)\n",
    "    df = load_dataset(name, split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "    df[\"answer_len\"] = [len(answer.split(\" \")) for answer in df[answer_column]]\n",
    "    df = df[df[\"answer_len\"] >= 20]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    num_samples = len(df)\n",
    "    avg_text_length = sum(len(text.split(\" \")) for text in df[answer_column]) / num_samples\n",
    "    max_text_length = max(len(text.split(\" \")) for text in df[answer_column])\n",
    "    min_text_length = min(len(text.split(\" \")) for text in df[answer_column])\n",
    "    std_text_length = np.std([len(text.split(\" \")) for text in df[answer_column]])\n",
    "    median_text_length = np.median([len(text.split(\" \")) for text in df[answer_column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "n_of_observations = 0\n",
    "for name, answer_column in english_datasets.items():\n",
    "    print(name)\n",
    "    df = load_dataset(name, split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "    n_of_observations += df.shape[0]\n",
    "    texts.extend(df[answer_column].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamini = load_dataset(\"MBZUAI/LaMini-instruction\", split=\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = lamini[lamini[\"instruction_source\"].str.contains(\"generated\") | lamini[\"instruction_source\"].str.contains(\"self_instruct\")]\n",
    "texts.extend(generated_texts[\"response\"].tolist())\n",
    "n_of_observations += generated_texts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_of_observations == len(texts)\n",
    "database_en = pd.DataFrame({\"text\": texts, \"lang\": \"en\"})\n",
    "database_en[\"answer_len\"] = [len(text.split(\" \")) for text in database_en[\"text\"]]\n",
    "database_en = database_en[database_en[\"answer_len\"] >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_datasets = {\"dkoterwa/alpaca_gpt_4_ar\": \"ar\",\n",
    "                  \"dkoterwa/alpaca_gpt_4_zh\": \"zh\",\n",
    "                  \"dkoterwa/alpaca_gpt_4_es\": \"es\",\n",
    "                  \"dkoterwa/alpaca_gpt_4_de\": \"de\",\n",
    "                  \"5CD-AI/Vietnamese-c-s-ale-alpaca-gpt4-data-gg-translated\": \"vi\"}\n",
    "dfs = []\n",
    "n_of_observations = 0\n",
    "for dataset, lang in other_datasets.items():\n",
    "    print(dataset)\n",
    "    df = load_dataset(dataset, split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "    n_of_observations += df.shape[0]\n",
    "    if lang != \"vi\":\n",
    "        df_with_lang = pd.DataFrame({\"text\": df[\"answer\"], \"lang\": lang})\n",
    "    else:\n",
    "        df_with_lang = pd.DataFrame({\"text\": df[\"output_vi\"], \"lang\": lang})\n",
    "        \n",
    "    dfs.append(df_with_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_other_languages = pd.concat(dfs)\n",
    "assert n_of_observations == len(database_other_languages)\n",
    "database_other_languages[\"answer_len\"] = [len(row[\"text\"].split(\" \")) if row[\"lang\"] != \"zh\" else len(jieba.lcut(row[\"text\"])) for _, row in database_other_languages.iterrows()]\n",
    "database_other_languages = database_other_languages[database_other_languages[\"answer_len\"] >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_en.reset_index(drop=True, inplace=True)\n",
    "database_other_languages.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_database = pd.concat([database_en, database_other_languages])\n",
    "full_database.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_database.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_database[\"lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_database.to_pickle(\"../data/database.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
