{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_fr = load_dataset(\"FreedomIntelligence/alpaca-gpt4-french\", split=\"train\").to_pandas()\n",
    "database_de = load_dataset(\"FreedomIntelligence/alpaca-gpt4-deutsch\", split=\"train\").to_pandas()\n",
    "database_es = load_dataset(\"FreedomIntelligence/alpaca-gpt4-spanish\", split=\"train\").to_pandas()\n",
    "database_ja = load_dataset(\"FreedomIntelligence/alpaca-gpt4-japanese\", split=\"train\").to_pandas()\n",
    "database_ko = load_dataset(\"FreedomIntelligence/alpaca-gpt4-korean\", split=\"train\").to_pandas()\n",
    "database_zh = load_dataset(\"FreedomIntelligence/alpaca-gpt4-chinese\", split=\"train\").to_pandas()\n",
    "\n",
    "databases = [database_fr, database_de, database_es, database_ja, database_ko, database_zh]\n",
    "database_df = pd.concat(databases)\n",
    "database_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df[\"chat_response\"] = [conv[1][\"value\"] for conv in database_df[\"conversations\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_en = load_dataset(\"vicgalle/alpaca-gpt4\", split=\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_responses  = database_df[\"chat_response\"].tolist() + database_en[\"output\"].tolist()\n",
    "len(list_of_responses) == len(database_df) + len(database_en)\n",
    "full_database_df = pd.DataFrame(list_of_responses, columns=['text'])\n",
    "full_database_df.reset_index(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws_en = load_dataset(\"maximedb/paws-x-all\", \"en\", split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "paws_de = load_dataset(\"maximedb/paws-x-all\", \"de\", split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "paws_fr = load_dataset(\"maximedb/paws-x-all\", \"fr\", split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "paws_es = load_dataset(\"maximedb/paws-x-all\", \"es\", split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "paws_ja = load_dataset(\"maximedb/paws-x-all\", \"ja\", split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "paws_ko = load_dataset(\"maximedb/paws-x-all\", \"ko\", split=\"train\", download_mode=\"force_redownload\").to_pandas()\n",
    "paws_zh = load_dataset(\"maximedb/paws-x-all\", \"zh\", split=\"train\", download_mode=\"force_redownload\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_paws(df):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.drop(\"id\", axis=1, inplace=True)\n",
    "    df = df[df[\"sentence1\"] != \"\"]\n",
    "    df.dropna(subset=\"sentence1\", inplace=True)\n",
    "    df.drop_duplicates(subset=\"sentence1\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws_en = preprocess_paws(paws_en)\n",
    "paws_de = preprocess_paws(paws_de)\n",
    "paws_fr = preprocess_paws(paws_fr)\n",
    "paws_es = preprocess_paws(paws_es)\n",
    "paws_ko = preprocess_paws(paws_ko)\n",
    "paws_ja = preprocess_paws(paws_ja)\n",
    "paws_zh = preprocess_paws(paws_zh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paws_en = paws_en[paws_en[\"label\"] == 1]\n",
    "# paws_de = paws_de[paws_de[\"label\"] == 1]\n",
    "# paws_fr = paws_fr[paws_fr[\"label\"] == 1]\n",
    "# paws_es = paws_es[paws_es[\"label\"] == 1]\n",
    "# paws_ja = paws_ja[paws_ja[\"label\"] == 1]\n",
    "# paws_ko = paws_ko[paws_ko[\"label\"] == 1]\n",
    "# paws_zh = paws_zh[paws_zh[\"label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws_list = [paws_en, paws_de, paws_fr, paws_es, paws_ja, paws_ko, paws_zh]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating faiss object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_database = model.encode(list_of_responses, batch_size=512, show_progress_bar=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(embeddings_database, axis=1, keepdims=True)\n",
    "norms[norms == 0] = 1\n",
    "normalized_embeddings_database = embeddings_database / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = normalized_embeddings_database.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(normalized_embeddings_database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws_dict = {\"en\": {\"dataset\": paws_en},\n",
    "            \"de\": {\"dataset\": paws_de},\n",
    "            \"fr\": {\"dataset\": paws_fr},\n",
    "            \"es\": {\"dataset\": paws_es},\n",
    "            \"ja\": {\"dataset\": paws_ja},\n",
    "            \"ko\": {\"dataset\": paws_ko},\n",
    "            \"zh\": {\"dataset\": paws_zh}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "k = 1\n",
    "for lang in tqdm(paws_dict):\n",
    "    nearest_neighbors = []\n",
    "    similarities = []\n",
    "    texts_to_test = paws_dict[lang][\"dataset\"][\"sentence1\"].tolist()\n",
    "    test_embeddings = model.encode(texts_to_test, batch_size=512, show_progress_bar=True)\n",
    "    norms = np.linalg.norm(test_embeddings, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    normalized_test_embeddings = test_embeddings / norms\n",
    "    test_dataset = TextDataset(normalized_test_embeddings)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        sim, index_df = index.search(batch.detach().cpu().numpy(), k)\n",
    "        indexes = [item for sublist in index_df for item in sublist]\n",
    "        sim_unsqueezed = [item for sublist in sim for item in sublist]\n",
    "        nearest_neighbors.extend(full_database_df.loc[indexes][\"text\"].tolist())\n",
    "        similarities.extend(sim_unsqueezed)\n",
    "    paws_dict[lang][\"nearest_neighbors\"] = nearest_neighbors\n",
    "    paws_dict[lang][\"similarities\"] = similarities        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_en = pd.DataFrame({\"text\": paws_dict[\"en\"][\"dataset\"][\"sentence1\"],\n",
    "                           \"nearest_neighbor\": paws_dict[\"en\"][\"nearest_neighbors\"],\n",
    "                           \"similarity\": paws_dict[\"en\"][\"similarities\"]})\n",
    "\n",
    "results_de = pd.DataFrame({\"text\": paws_dict[\"de\"][\"dataset\"][\"sentence1\"],\n",
    "                           \"nearest_neighbor\": paws_dict[\"de\"][\"nearest_neighbors\"],\n",
    "                           \"similarity\": paws_dict[\"de\"][\"similarities\"]})\n",
    "\n",
    "results_fr = pd.DataFrame({\"text\": paws_dict[\"fr\"][\"dataset\"][\"sentence1\"],\n",
    "                           \"nearest_neighbor\": paws_dict[\"fr\"][\"nearest_neighbors\"],\n",
    "                           \"similarity\": paws_dict[\"fr\"][\"similarities\"]})\n",
    "\n",
    "results_es = pd.DataFrame({\"text\": paws_dict[\"es\"][\"dataset\"][\"sentence1\"],\n",
    "                           \"nearest_neighbor\": paws_dict[\"es\"][\"nearest_neighbors\"],\n",
    "                           \"similarity\": paws_dict[\"es\"][\"similarities\"]})\n",
    "\n",
    "results_ja = pd.DataFrame({\"text\": paws_dict[\"ja\"][\"dataset\"][\"sentence1\"],\n",
    "                           \"nearest_neighbor\": paws_dict[\"ja\"][\"nearest_neighbors\"],\n",
    "                           \"similarity\": paws_dict[\"ja\"][\"similarities\"]})\n",
    "\n",
    "results_ko = pd.DataFrame({\"text\": paws_dict[\"ko\"][\"dataset\"][\"sentence1\"],\n",
    "                           \"nearest_neighbor\": paws_dict[\"ko\"][\"nearest_neighbors\"],\n",
    "                           \"similarity\": paws_dict[\"ko\"][\"similarities\"]})\n",
    "\n",
    "results_zh = pd.DataFrame({\"text\": paws_dict[\"zh\"][\"dataset\"][\"sentence1\"],\n",
    "                           \"nearest_neighbor\": paws_dict[\"zh\"][\"nearest_neighbors\"],\n",
    "                           \"similarity\": paws_dict[\"zh\"][\"similarities\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(results):\n",
    "    results[\"prediction\"] = [1 if sim > 0.9 else 0 for sim in results[\"similarity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(results_en)\n",
    "make_predictions(results_de)\n",
    "make_predictions(results_fr)\n",
    "make_predictions(results_es)\n",
    "make_predictions(results_ja)\n",
    "make_predictions(results_ko)\n",
    "make_predictions(results_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_false_positive_ratio(predictions):\n",
    "    return predictions[predictions == 1].count() * 100 / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_en = calculate_false_positive_ratio(results_en[\"prediction\"])\n",
    "fpr_de = calculate_false_positive_ratio(results_de[\"prediction\"])\n",
    "fpr_es = calculate_false_positive_ratio(results_es[\"prediction\"])\n",
    "fpr_fr = calculate_false_positive_ratio(results_fr[\"prediction\"])\n",
    "fpr_ja = calculate_false_positive_ratio(results_ja[\"prediction\"])\n",
    "fpr_ko = calculate_false_positive_ratio(results_ko[\"prediction\"])\n",
    "fpr_zh = calculate_false_positive_ratio(results_zh[\"prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magisterka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
