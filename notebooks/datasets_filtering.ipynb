{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominik/venvs/master_thesis/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/dominik/venvs/master_thesis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import  tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> OASST <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oass_train = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").to_pandas()\n",
    "oass_valid = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").to_pandas()\n",
    "oass_full = pd.concat([oass_train, oass_valid,])\n",
    "oass_full.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oass_full[\"review_result\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lang: en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [02:16<11:23, 136.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lang: ar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [02:16<03:45, 56.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lang: de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [02:29<01:48, 36.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lang: es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [03:33<01:34, 47.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lang: vi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [03:34<00:30, 30.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lang: zh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:52<00:00, 38.76s/it]\n"
     ]
    }
   ],
   "source": [
    "oass_train = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").to_pandas()\n",
    "oass_valid = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").to_pandas()\n",
    "oass_full = pd.concat([oass_train, oass_valid,])\n",
    "oass_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "needed_langs = [\"en\", \"ar\", \"de\", \"es\", \"vi\", \"zh\"]\n",
    "rows = []\n",
    "for lang in tqdm(needed_langs):\n",
    "    print(f\"Processing lang: {lang}\")\n",
    "    filtered_df = oass_full[(oass_full[\"lang\"] == lang) & (oass_full[\"role\"] == \"assistant\")]\n",
    "    for i, answer in filtered_df.iterrows():\n",
    "        query = oass_full[oass_full[\"message_id\"] == answer[\"parent_id\"]][\"text\"].iloc[0]\n",
    "        rows.append([answer[\"lang\"], answer[\"message_id\"], answer[\"parent_id\"], answer[\"user_id\"], answer[\"created_date\"], query, answer[\"text\"], answer[\"review_count\"]])\n",
    "        \n",
    "filtered_dataset = pd.DataFrame(rows, columns=[\"lang\", \"message_id\", \"parent_id\", \"user_id\", \"created_date\", \"query\", \"answer\", \"review_count\"])\n",
    "filtered_dataset.drop_duplicates(subset=\"answer\", inplace=True)\n",
    "filtered_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(filtered_dataset)\n",
    "hf_dataset.push_to_hub(\"dkoterwa/oasst2_filtered_retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MKQA <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkqa = load_dataset(\"mkqa\", split=\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 30089.21it/s]\n"
     ]
    }
   ],
   "source": [
    "needed_langs = [\"en\", \"ar\", \"de\", \"es\", \"vi\", \"zh_cn\"]\n",
    "rows = []\n",
    "for i, row in tqdm(mkqa.iterrows(), total=mkqa.shape[0]):\n",
    "    for lang in needed_langs:\n",
    "        rows.append([lang, row[\"example_id\"], row[\"queries\"][lang], row[\"answers\"][lang][0][\"text\"]])\n",
    "        \n",
    "filtered_dataset = pd.DataFrame(rows, columns=[\"lang\", \"example_id\", \"query\", \"answer\"])\n",
    "filtered_dataset.dropna(inplace=True)\n",
    "filtered_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 41/41 [00:00<00:00, 1572.09ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.72s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dkoterwa/mkqa_filtered/commit/acdcc09e2035ce21e403f81303826a9d0a6697be', commit_message='Upload dataset', commit_description='', oid='acdcc09e2035ce21e403f81303826a9d0a6697be', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = Dataset.from_pandas(filtered_dataset)\n",
    "hf_dataset.push_to_hub(\"dkoterwa/mkqa_filtered\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
