{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import  tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> OASST <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oass_train = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").to_pandas()\n",
    "oass_valid = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").to_pandas()\n",
    "oass_full = pd.concat([oass_train, oass_valid,])\n",
    "oass_full.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oass_full[\"review_result\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oass_train = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").to_pandas()\n",
    "oass_valid = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").to_pandas()\n",
    "oass_full = pd.concat([oass_train, oass_valid,])\n",
    "oass_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "needed_langs = [\"en\", \"ar\", \"de\", \"es\", \"vi\", \"zh\"]\n",
    "rows = []\n",
    "for lang in tqdm(needed_langs):\n",
    "    print(f\"Processing lang: {lang}\")\n",
    "    filtered_df = oass_full[(oass_full[\"lang\"] == lang) & (oass_full[\"role\"] == \"assistant\")]\n",
    "    for i, answer in filtered_df.iterrows():\n",
    "        query = oass_full[oass_full[\"message_id\"] == answer[\"parent_id\"]][\"text\"].iloc[0]\n",
    "        rows.append([answer[\"lang\"], answer[\"message_id\"], answer[\"parent_id\"], answer[\"user_id\"], answer[\"created_date\"], query, answer[\"text\"], answer[\"review_count\"]])\n",
    "        \n",
    "filtered_dataset = pd.DataFrame(rows, columns=[\"lang\", \"message_id\", \"parent_id\", \"user_id\", \"created_date\", \"query\", \"answer\", \"review_count\"])\n",
    "filtered_dataset.drop_duplicates(subset=\"answer\", inplace=True)\n",
    "filtered_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(filtered_dataset)\n",
    "hf_dataset.push_to_hub(\"dkoterwa/oasst2_filtered_retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MKQA <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkqa = load_dataset(\"mkqa\", split=\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_langs = [\"en\", \"ar\", \"de\", \"es\", \"vi\", \"zh_cn\"]\n",
    "rows = []\n",
    "for i, row in tqdm(mkqa.iterrows(), total=mkqa.shape[0]):\n",
    "    for lang in needed_langs:\n",
    "        rows.append([lang, row[\"example_id\"], row[\"queries\"][lang], row[\"answers\"][lang][0][\"text\"]])\n",
    "        \n",
    "filtered_dataset = pd.DataFrame(rows, columns=[\"lang\", \"example_id\", \"query\", \"answer\"])\n",
    "filtered_dataset.dropna(inplace=True)\n",
    "filtered_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(filtered_dataset)\n",
    "hf_dataset.push_to_hub(\"dkoterwa/mkqa_filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MLQA <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mlqa(subset_name):\n",
    "    dataset_valid = load_dataset(\"mlqa\", subset_name, split=\"validation\").to_pandas()\n",
    "    dataset_test  = load_dataset(\"mlqa\", subset_name, split=\"test\").to_pandas()\n",
    "    full_dataset = pd.concat([dataset_valid, dataset_test])\n",
    "    full_dataset.reset_index(drop=True, inplace=True)\n",
    "    return full_dataset\n",
    "\n",
    "needed_langs = [\"mlqa.en.en\", \"mlqa.de.de\", \"mlqa.ar.ar\", \"mlqa.es.es\", \"mlqa.vi.vi\", \"mlqa.zh.zh\"]\n",
    "datasets = []\n",
    "for lang in tqdm(needed_langs):\n",
    "    dataset = download_mlqa(lang)\n",
    "    dataset[\"lang\"] = lang.split(\".\")[2]\n",
    "    datasets.append(dataset)\n",
    "    \n",
    "full_mlqa = pd.concat(datasets)\n",
    "full_mlqa.reset_index(drop=True, inplace=True)\n",
    "full_mlqa[\"answer\"] = [answer_dict[\"text\"][0] for answer_dict in full_mlqa[\"answers\"]]\n",
    "full_mlqa.drop(\"answers\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(full_mlqa)\n",
    "hf_dataset.push_to_hub(\"dkoterwa/mlqa_filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Database datasets <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"FreedomIntelligence/alpaca-gpt4-deutsch\": \"de\",\n",
    "            \"FreedomIntelligence/alpaca-gpt4-spanish\": \"es\",\n",
    "            \"FreedomIntelligence/alpaca-gpt4-chinese\": \"zh\",\n",
    "            \"FreedomIntelligence/alpaca-gpt4-arabic\": \"ar\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/teamspace/studios/this_studio/.cache/huggingface/datasets/FreedomIntelligence___json/FreedomIntelligence--alpaca-gpt4-deutsch-105d0824df499611/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You need to provide a `token` or be logged in to Hugging Face with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m hf_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(dataset)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mhf_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdkoterwa/alpaca_gpt_4_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/arrow_dataset.py:5380\u001b[0m, in \u001b[0;36mDataset.push_to_hub\u001b[0;34m(self, repo_id, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_shards \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to push_to_hub: please specify either max_shard_size or num_shards, but not both.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5378\u001b[0m     )\n\u001b[0;32m-> 5380\u001b[0m repo_id, split, uploaded_size, dataset_nbytes, repo_files, deleted_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_push_parquet_shards_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5382\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbranch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5388\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_external_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5390\u001b[0m organization, dataset_name \u001b[38;5;241m=\u001b[39m repo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5391\u001b[0m info_to_dump \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/arrow_dataset.py:5185\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5182\u001b[0m token \u001b[38;5;241m=\u001b[39m token \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m HfFolder\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m   5184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   5186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to provide a `token` or be logged in to Hugging Face with `huggingface-cli login`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5187\u001b[0m     )\n\u001b[1;32m   5189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5190\u001b[0m     split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: You need to provide a `token` or be logged in to Hugging Face with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "for dataset_name, language in datasets.items():\n",
    "    dataset = load_dataset(dataset_name, split=\"train\").to_pandas()\n",
    "    dataset[\"answer\"] = [conversation[1][\"value\"] for conversation in dataset[\"conversations\"]]\n",
    "    dataset[\"instruction\"] = [conversation[0][\"value\"] for conversation in dataset[\"conversations\"]]\n",
    "    dataset.drop(\"conversations\", axis=1, inplace=True)\n",
    "    hf_dataset = Dataset.from_pandas(dataset)\n",
    "    hf_dataset.push_to_hub(f\"dkoterwa/alpaca_gpt_4_{language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
